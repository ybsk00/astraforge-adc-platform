# [AntiGravity 작업지시] 사용자 데이터 업로드 — 개인 전용 MVP 구현 (즉시 개발)

## 0) 목표(이번 스프린트 범위)
로그인한 **개별 사용자(user_id)** 가 자신의 데이터를 업로드하여 플랫폼에서 **후보 등록/실행(Scoring/Rules/Protocols)** 흐름에 사용할 수 있도록 백엔드를 즉시 구현한다.

- 정책: **개인 전용(Private by default)**  
- 공유 기능: **이번 범위에서 미구현(추후)**  
- 화면/메뉴: 기존 사용자 UI에 존재하는 “데이터 업로드” 기능을 **404 없이 정상 동작**하도록 만든다.
- 업로드 대상: **Candidate 업로드(CSV) 1차** + (옵션) 원본 파일 저장 및 업로드 내역 조회

---

## 1) 핵심 결정(고정)
### 1.1 데이터 소유권 / 접근 정책
- 모든 업로드 데이터는 `owner_user_id = auth.user.id`로 귀속
- 접근 제어: **owner만 조회/다운로드/삭제 가능**
- 공유(다른 사용자 접근) 기능은 없음

### 1.2 저장 전략
- 파일 원본: Storage (Supabase Storage 또는 현재 프로젝트 스토리지 정책)
- 메타데이터/파싱결과: DB 테이블
- 업로드→파싱→candidate 생성은 Worker로 처리(또는 API 동기 처리 MVP)

---

## 2) 구현 산출물(필수)
### 2.1 DB 스키마 추가(마이그레이션)
#### 2.1.1 uploads (업로드 메타)
- `id` UUID PK
- `owner_user_id` UUID NOT NULL
- `type` TEXT NOT NULL  # candidate_csv | experiment_csv(추후) | doc_pdf(추후)
- `filename` TEXT NOT NULL
- `storage_bucket` TEXT NOT NULL
- `storage_key` TEXT NOT NULL
- `mime_type` TEXT
- `size_bytes` BIGINT
- `checksum` TEXT NULL
- `status` TEXT NOT NULL DEFAULT 'uploaded'  # uploaded | parsing | parsed | failed
- `error_message` TEXT NULL
- `created_at` TIMESTAMPTZ DEFAULT now()

#### 2.1.2 candidates (개인 후보 카탈로그)
(기존 테이블이 있으면 그대로 사용하되, 없으면 아래 생성)
- `id` UUID PK
- `owner_user_id` UUID NOT NULL
- `name` TEXT NOT NULL
- `metadata` JSONB DEFAULT '{}'
- `features` JSONB DEFAULT '{}'  # DAR, LogP, AggRisk 등 정규화된 입력
- `source_upload_id` UUID NULL REFERENCES uploads(id)
- `created_at` TIMESTAMPTZ DEFAULT now()

#### 2.1.3 candidate_inputs (원본 행 저장: 추적성)
- `id` UUID PK
- `candidate_id` UUID REFERENCES candidates(id)
- `owner_user_id` UUID NOT NULL
- `raw_row` JSONB NOT NULL  # CSV row 그대로 저장
- `row_index` INT NOT NULL
- `created_at` TIMESTAMPTZ DEFAULT now()

> NOTE: 기존에 유사 테이블이 있다면 중복 생성하지 말고, `owner_user_id` 중심으로 연결만 한다.

---

## 3) RLS(필수)
Supabase를 쓰는 전제에서:
- `uploads`: `owner_user_id = auth.uid()`
- `candidates`: `owner_user_id = auth.uid()`
- `candidate_inputs`: `owner_user_id = auth.uid()`

권장 정책:
- SELECT/INSERT/UPDATE/DELETE 모두 동일 조건
- Storage도 동일한 경로 규칙(개인 폴더 구조) 적용

---

## 4) 스토리지 경로 규칙(개인 전용)
- Bucket: `user-uploads` (없으면 생성)
- Key: `users/{user_id}/{yyyy}/{mm}/{upload_id}/{filename}`

장점:
- 사용자별 격리 명확
- 삭제/정리 작업 간단

---

## 5) API 설계(필수 엔드포인트)
### 5.1 업로드 준비(Pre-sign)
`POST /api/uploads/presign`
- 입력: `{ type, filename, mime_type, size_bytes }`
- 출력: `{ upload_id, bucket, key, presigned_url, headers(optional) }`
- 처리:
  - `uploads` 레코드 status='uploaded'로 생성
  - presigned url 발급 후 반환
  - Audit event: `upload.created`

### 5.2 업로드 커밋(Commit)
`POST /api/uploads/commit`
- 입력: `{ upload_id }`
- 처리:
  - 업로드 메타 업데이트(파일 크기/체크섬 가능하면)
  - status='parsing'로 변경
  - Worker/Job 큐에 parse job enqueue
  - Audit event: `upload.committed`

### 5.3 업로드 목록/상태
`GET /api/uploads?type=candidate_csv`
- 출력: 업로드 목록(내 것만)

### 5.4 Candidate Import 실행(옵션: commit 단계에서 자동 실행해도 됨)
`POST /api/candidates/import`
- 입력: `{ upload_id }`
- 처리:
  - CSV 파싱
  - row별 candidate 생성
  - candidate_inputs 저장
  - uploads.status='parsed'
  - Audit event: `candidate.imported`

> MVP 권장 흐름:
- commit 시 자동으로 import job을 돌리고, 별도 import API는 생략 가능

---

## 6) Worker 통합(필수)
파일: `run_execute_job.py` 또는 현재 Worker 엔트리포인트에 통합

### 6.1 새 Job: `parse_candidate_csv_job`
입력: `upload_id`
처리:
1) 업로드 레코드 조회 (owner_user_id 검증)
2) storage_key로 파일 다운로드
3) CSV 파싱
4) row validation (최소 컬럼 검증)
5) candidates 생성 + candidate_inputs 저장
6) 업로드 상태 업데이트:
   - 성공: `parsed`
   - 실패: `failed` + error_message
7) Audit 기록:
   - `upload.parsed` 또는 `upload.failed`

### 6.2 CSV 컬럼 규격(MVP)
최소 허용 컬럼(없으면 validation error):
- `name` (candidate name)
선택 컬럼(있으면 features에 매핑):
- `DAR`, `LogP`, `AggRisk`, `H_patch`, `CLV`, `INT` 등(프로젝트 문서 기준)

매핑 규칙:
- 숫자형은 float로 캐스팅 실패 시 NULL
- 원본은 `candidate_inputs.raw_row`에 그대로 저장

---

## 7) UI 연결(사용자 기능 유지, 404 제거)
- 기존 “데이터 업로드” 메뉴/페이지의 라우트가 `[locale]` 내부로 이동되어 있어야 함
- 업로드 페이지에서 구현해야 하는 최소 기능:
  1) 파일 선택(CSV)
  2) 업로드 진행 표시
  3) 업로드 완료 후 “파싱 중/완료/실패” 상태 표시
  4) 완료 시 candidate 목록 페이지로 이동 또는 “후보 생성 완료 N건” 표시

> 이번 작업은 “사용자 기능 구현”이므로 **어드민 기능 수정은 제외**.

---

## 8) 감사(Audit) 연동(필수)
Audit Service가 이미 미구현이면 이번 작업에서 함께 구현하거나,
최소한 DB에 `audit_events` 기록만 남겨도 됨.

기록 이벤트:
- `upload.created`
- `upload.committed`
- `upload.parsed`
- `upload.failed`
- `candidate.imported`

metadata 마스킹:
- 파일명/키는 허용
- token/secret 등은 마스킹

---

## 9) 테스트(필수)
### 9.1 자동 테스트(pytest)
- `test_upload_presign_owner_only`
- `test_upload_commit_enqueues_job`
- `test_parse_candidate_csv_creates_candidates`
- `test_rls_blocks_other_user_access` (가능하면)

### 9.2 수동 검증 시나리오
1) A 사용자 로그인 → CSV 업로드 → candidates 생성 확인
2) B 사용자 로그인 → A의 업로드/후보 접근 시도 → 차단 확인
3) CSV 오류(컬럼 누락/깨진 파일) → uploads.status='failed' 확인

---

## 10) 완료 정의(DoD)
- “데이터 업로드” 클릭 시 404가 발생하지 않고 업로드/파싱이 완료된다
- 업로드된 후보가 Candidate Catalog에 생성된다
- 다른 사용자는 내 업로드/후보 데이터를 조회할 수 없다(RLS)
- 업로드/파싱 이벤트가 audit_events에 기록된다
- 최소 테스트/수동 검증 시나리오 통과

---

## 11) 파일/디렉토리(권장 구조)
- `services/engine/app/api/uploads.py` (또는 현 API 구조에 맞춰)
- `services/engine/app/jobs/parse_candidate_csv_job.py`
- `services/engine/app/services/storage_service.py` (프리사인/다운로드 공통)
- `services/engine/app/services/audit_service.py` (없으면 최소 구현)
- `migrations/xxxx_create_uploads_candidates.sql`

---

## 12) 작업 순서(실행 플로우)
1) DB + RLS 마이그레이션
2) Storage bucket/경로 규칙 확정 + presign API 구현
3) commit API 구현 + job enqueue
4) parse_candidate_csv_job 구현
5) UI 연결(업로드 페이지에서 presign→upload→commit)
6) audit 기록 연결
7) 테스트/수동 검증

---
