# Worker 시스템 설명서 — Manager / Dispatcher / Jobs 구조
버전: v1.0  
작성일: 2026-01-13  
목적: ADC 플랫폼의 워커(Worker) 아키텍처를 **역할 기반(Manager/Dispatcher/Jobs)**으로 정리하고, 각 파일의 책임/데이터 흐름/확장 포인트를 문서화하여 향후 모듈별 개선 작업의 기준으로 사용한다.

---

## 1) 워커 시스템 개요(Why)
워커는 “사용자/관리자 요청”을 비동기 작업(Job)으로 처리한다.  
웹(UI/API)은 요청을 등록하고 상태를 조회하는 역할에 집중하며, **데이터 수집·정규화·계산·후보 생성·보고서 생성**은 워커가 수행한다.

워커는 크게 3계층으로 구성된다.

1. **관리자(Manager)**: 프로세스 진입점, 연결, Job 등록, DB 폴링  
2. **배분자(Dispatcher)**: 어떤 Job을 실행할지 결정(관제탑)  
3. **작업자(Jobs)**: 실제 데이터 적재/수집/동기화/후보 생성 수행

---

## 2) 핵심 관리 및 실행(Core Layer)
### 2.1 `worker.py` — 관리자(Manager)
#### 역할
- 워커 프로세스의 **진입점(Entry Point)**
- Redis / Supabase 연결 초기화
- 실행 가능한 Job 함수들을 등록(registry)
- 주기적으로 DB를 폴링하여 대기 중인 작업을 가져옴 (`poll_db_jobs`)
- 가져온 작업을 Dispatcher에게 전달하여 실행을 위임

#### 책임 경계(권장)
- “무엇을 할지” 판단하지 않는다. (Dispatcher 책임)
- “어떻게 할지” 구현하지 않는다. (Job 책임)
- 해야 할 일:
  - 연결/초기화
  - Job registry 관리
  - DB polling 및 작업 수명주기 상태 업데이트(queued/running/done/failed)
  - 공통 로깅/에러 핸들링/재시도 정책(최소)

#### 입출력(개념)
- 입력: `connector_runs` 등 “실행 대기 작업 레코드”
- 출력: 실행 트리거(Dispatcher 호출) 및 상태 업데이트

---

### 2.2 `connector_executor.py` — 배분자(Dispatcher)
#### 역할
- `worker.py`가 가져온 작업(`connector_runs`)을 분석
- 작업의 `connector_type`(api / db / system 등) 및 세부 설정을 읽고
- 해당 타입에 맞는 Job을 선택하여 실행(호출)하는 관제탑

#### 책임 경계(권장)
- “어떤 Job을 실행할지”는 여기서 결정
- 하지만 “Job 내부 로직(수집/적재/추론)”은 알지 못한다.
- 해야 할 일:
  - 커넥터 타입별 Job 매핑 테이블 유지
  - config 파싱 및 파라미터 전달
  - 실행 결과/에러를 표준 포맷으로 `connector_runs`에 기록
  - 공통 타임아웃/재시도 정책 적용(가능하면)

#### 권장 개선 포인트
- 커넥터 타입이 늘어나도 분기문이 폭증하지 않도록
  - “타입→함수” 매핑 딕셔너리/레지스트리 방식 고정
- 실행 로그를 “표준 구조(JSON)”로 남겨
  - Admin UI에서 실행 내역 분석이 가능하도록 함

---

## 3) 시드 및 골든셋 데이터 생성(Seeding & Golden Set Layer)
초기 데이터 구축과 “정답지/후보군” 생성은 엔진의 신뢰도를 좌우한다.

### 3.1 `seed_job.py` — 기초 데이터 시더(Base Seeder)
#### 역할
- `component_catalog`에 기초 데이터(Target, Antibody, Payload, Linker)를 적재
- 정적 JSON(예: `targets_seed_200.json`) 및 코드 내 seed 데이터를 DB에 반영
- 시스템 전체 파이프라인의 기준 사전(Canonical Dictionary)을 구축

#### 특징(현 상태)
- Partial unique index 이슈로 upsert 대신 select→insert/update로 안정화
- “JSON 우선 merge” 등 병합 로직 강화 계획 진행 중

#### 권장 책임 경계(To-Be)
- seed_job은 **정규화된 canonical 엔티티**만 만든다.
- “근거(evidence) 수집/추천/점수화”는 하지 않는다.
- 반드시 포함되어야 하는 필드(특히 payload/linker):
  - `synonyms`
  - (payload) `smiles` 등 구조 데이터(RDKit 계산 전제)
  - (linker) `linker_type`, `trigger` 등 설계 분류 필드

---

### 3.2 `golden_seed_job.py` — 골든셋 빌더(Golden Candidate Builder)
#### 역할
- ClinicalTrials.gov에서 임상 데이터를 가져와 **고품질 후보(Golden Candidate)** 생성
- 텍스트에서 ADC 구성요소(항체/링커/페이로드)를 추출 및 매핑
- 점수를 매겨 Top 100 후보를 선별(또는 raw/final 분리)

#### 특징(현 상태)
- 최근 개선 포인트:
  - Mock 제거 방향
  - 추출 패턴 강화(intervention, suffix 기반 추론)
  - catalog 매칭 실패 시 휴리스틱 보조

#### 권장 책임 경계(To-Be)
- 1) RAW 수집 저장(전량)  
- 2) 매핑/정규화(component_catalog 연결, confidence 포함)  
- 3) FINAL 후보 승격(Top N, is_final)  
- golden_seed_job은 “임상 기반 후보 pool 생성”까지 담당하며,
- RDKit 계산/전임상 정형 근거/추천 엔진(100점)은 별도 Job으로 분리하는 것이 안정적이다.

---

### 3.3 `rag_seed_job.py` — AI 시더(RAG 기반)
#### 역할
- PubMed 문헌 벡터 DB를 검색하여 문헌 근거가 있는 ADC 후보를 발굴
- AI(LLM) + 벡터 검색으로 후보를 생성/추천

#### 특징
- 정형 데이터(임상/전임상 DB)로 커버되지 않는 최신/니치 영역을 보강
- 다만 “구조(SMILES) 추출”은 문헌 텍스트만으로 불안정할 수 있음

#### 권장 책임 경계(To-Be)
- rag_seed_job은 “후보 제안 + 근거 문장/요약 + 출처” 중심으로 설계
- 구조 데이터(SMILES)는:
  - PubChem/ChEMBL 매핑 등 정형 소스 연동으로 보강
  - 또는 관리자 검수 루프를 전제
- 결과는 golden 후보 풀에 “추가 입력”으로 들어가되,
  - 최종 승격은 validator/스코어링을 통과해야 함

---

## 4) 외부 데이터 수집 및 동기화(Data Collection & Sync Layer)
“근거 기반 설계 엔진”의 자료를 채우는 레이어.  
Seed로 만든 기준 사전(component_catalog)을 바탕으로 batch_mode로 효율화한다.

### 4.1 `meta_sync_job.py` — 배치 동기화(Batch Enrichment)
#### 역할
- OpenTargets, HPA, ChEMBL, PubChem 등 대형 외부 DB와 동기화
- batch_mode를 지원하여 “우리 Catalog에 존재하는 대상”만 선별 수집

#### 특징
- 비용/시간 폭증을 막는 핵심 장치
- target_profiles(통합 프로필)에 엔리치먼트 업데이트 방식 선호

#### 권장 확장 포인트
- payload/linker까지 확장 시:
  - 구조/활성/물성 정보의 정규화 스키마 정립 필요

---

### 4.2 `clinical_job.py` — 임상 데이터 수집(Clinical Raw Ingestion)
#### 역할
- ClinicalTrials.gov, OpenFDA API 호출
- 원본 임상 데이터를 수집/저장

#### 권장 포지셔닝
- `golden_seed_job.py`가 후보 생성 로직까지 가진다면,
  - `clinical_job.py`는 “원본 수집/캐시”를 담당하고,
  - golden_seed_job은 캐시를 읽어 가공하는 구조가 더 견고함
- (현재 구조가 다르더라도) 장기적으로 분리 권장

---

### 4.3 `pubmed_job.py` — 문헌 수집(Literature Ingestion)
#### 역할
- PubMed에서 키워드 기반 논문 검색/수집
- 후속 처리(청킹/임베딩/RAG)를 위한 원문/메타데이터 확보

#### 권장 포지셔닝
- “검색/수집”과 “청킹/임베딩”은 분리 권장
- evidence_items 또는 문헌 테이블에 출처/초록/PMID 등을 저장하고
- 벡터 DB(pgvector)는 별도 파이프라인에서 생성

---

### 4.4 `uniprot_job.py` — 단백질 정보 수집(Target Enrichment)
#### 역할
- UniProt API를 통해 타겟 단백질 상세 정보 수집
- target_profiles의 기본 뼈대를 채우는 역할

#### 권장 포지셔닝
- 타겟은 seed+uniprot을 기반으로 표준화가 가능하므로,
- 전체 파이프라인의 “ID 정합성”을 강화하는 핵심 Job으로 유지

---

## 5) 데이터 흐름(Workflow) 요약
### 5.1 이벤트 흐름(실행)
1. `worker.py`가 주기적으로 DB 폴링(`poll_db_jobs`)하여 대기 작업을 찾는다.
2. 작업이 있으면 `connector_executor.py`에게 전달한다.
3. `connector_executor.py`는 작업의 타입/설정에 따라 적절한 Job을 실행한다.
4. Job 실행 결과(성공/실패/로그/산출물)는 DB에 기록된다.
5. Admin/사용자 UI는 DB 상태를 조회하여 진행 상황과 결과를 보여준다.

### 5.2 데이터 흐름(데이터 생성 관점)
- Seed: `seed_job.py` → component_catalog(기준 사전)
- Enrichment: `meta_sync_job.py`, `uniprot_job.py`, `pubmed_job.py`, `clinical_job.py` → 근거/프로필/원천 데이터 축적
- Golden 후보: `golden_seed_job.py`(+ `rag_seed_job.py`) → 후보 pool 생성(Top N)
- (후속 확장) RDKit/전임상/추천/리포트 Job → 점수화 및 보고서 생성

---

## 6) 운영상 권장 규칙(품질/신뢰 확보)
1) RAW와 FINAL을 분리한다.
- 수집된 원천 데이터는 RAW로 전량 저장
- 사용자/보고서에 반영되는 데이터는 FINAL(검증/정규화 통과)만 사용

2) 매핑 confidence를 기록한다.
- 임상 텍스트에서 추출한 항체/페이로드/링커는 오인식 가능
- confidence가 낮으면 FINAL 승격 금지 또는 경고 부여

3) Seed는 “정규화된 사전”이며 추천 엔진이 아니다.
- 추천/점수화는 별도 Job으로 분리(유지보수/테스트/신뢰성)

4) 로그는 구조화(JSON)로 남긴다.
- Admin에서 “왜 실패했는지, 왜 이 후보가 뽑혔는지”를 추적 가능해야 함

---

## 7) 향후 확장 포인트(엔진화 로드맵 관점)
현재 워커 구조를 유지하면서, “레고” 인상을 제거하고 설계 엔진을 완성하기 위한 권장 추가 Job(향후 요청 시 모듈별 MD로 분리):

- `rdkit_features_job.py`
  - payload/linker SMILES 기반 descriptor/fingerprint/유사도 산출
- `preclinical_evidence_job.py`
  - ChEMBL/PubChem 등 정형 전임상 근거 수집/정규화
- `recommendation_job.py`
  - 100점 스코어링 + Top-K 조합 추천 + 근거 번들 생성
- `report_job.py`
  - 추천 결과를 PDF/HTML 보고서로 자동 생성

---

## 8) 파일-역할 매트릭스(요약)
### Core
- worker.py: Manager(진입점/연결/폴링/등록)
- connector_executor.py: Dispatcher(타입별 Job 선택/실행 관제)

### Seeding & Golden
- seed_job.py: base seeding(component_catalog)
- golden_seed_job.py: clinical 기반 golden 후보 생성(Top N)
- rag_seed_job.py: PubMed RAG 기반 후보 발굴(근거 중심)

### Data Collection & Sync
- meta_sync_job.py: 대형 DB batch enrichment
- clinical_job.py: 임상 원본 수집
- pubmed_job.py: 문헌 원본 수집
- uniprot_job.py: 단백질/타겟 정보 수집

---

문서 끝.
